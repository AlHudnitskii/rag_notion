# 6. Sharding

Категория: NoSQL & General theory
Статус: Готово

**Шардинг** — это горизонтальное разделение данных на независимые фрагменты (shards), где каждый шар хранит *часть* данных с целью повысить масштабируемость и производительность, снизить нагрузку на один сервер.

**Техническая аналогия** 

Если репликация — это копирование всей книги, то шардинг — это разделение книги на главы и распределение их между разными библиотеками.

### Routing (proxy / driver)

Общая архитектура: клиент — роутер — шард.

Чтобы приложение могло понять, в какой шард отправить запрос, нужна особая архитектура.

**Клиент**: Программа на стороне пользователя, которая отправляет стандартный запрос к базе данных (например, SELECT * FROM users WHERE user_id = 123).

**Маршрутизатор запросов** или роутер (Query Router): Это что-то вроде посредника между клиентом и шардом, который выполняет роль диспетчера. Он принимает запрос, при помощи sharding ключа определяет, что это за данные, в какой шард и с какой целью их надо отправить. Далее он отправляет запрос на нужный шард.

**Шард (Shard)**: Получает запрос, выполняет и возвращает результат обратно маршрутизатору, который затем передаёт его клиенту.

![image.png](image%20620.png)

### Стратегия 1 -  Client-side routing

Логика маршрутизации зашита в клиентском библиотечном драйвере.

**Как работает:**

1. Клиент знает «ключ шардинга» (shard key).
2. Клиент знает map: shard_key → shard_id.
3. Клиент сам вычисляет шард → и отправляет запрос напрямую.

**Пример:** Cassandra / Scylla / DynamoDB

**Плюсы:**

- Нет промежуточных прокси → максимум скорости.
- Меньше точек отказа.

**Минусы:**

- Сложнее обновлять routing-таблицу (требует синхронизации)
- При изменениях topology (rebalancing) нужно обновлять всех клиентов.

### Стратегия 2 — Proxy routing

Драйвер общается не с шардами, а с одним endpoint.

**Пример:** MySQL Vitess, Yugabyte Proxy, Elastic coordinating node.

**Плюсы:**

- Прокси знает всё о шардах → клиенты максимально простые.
- Изменение топологии прозрачно для клиентов.

**Минусы:**

- Прокси может стать bottleneck.
- Добавляет задержку (одно лишнее сетевое «плечо»).

### Стратегия 3 — Coordinator node

Запрос приходит на один узел → он определяет шард → он же переадресует.

**Пример:**

MongoDB mongos.

**Плюсы:**

- Легко масштабировать — можно поднять много coordinating nodes.
- Клиенту не нужен routing logic.

**Минусы:**

- Coordinator нагружается heavy queries.
- Возможны узкие места при cross-shard операциях.

### Rebalancing (перераспределение данных)

Когда один шард переполнен (hotspot), нужно «разгрузить» кластер.

## **Три типичных причины ребалансинга**

1. **Hot shard** — какой-то shard_key слишком популярен
    
    (например, "user_id = USA").
    
2. **Добавление нового шарда** — нужно распределить данные равномерно.
3. **Удаление/падение шарда** — нужно перенести данные соседям.

## **Стратегии ребалансинга**

1. Range-based sharding - Данные распределены по диапазонам (например, user_id 1–10000 → shard1)
2. Hash-based sharding  **-** Индекс: `hash(user_id) % shard_count`

Проблема: при добавлении нового шарда → хеши меняются.

Решение:

- consistent hashing (см. следующий раздел)
- виртуальные ноды
1. Directory-based sharding вводит отдельный справочник (каталог) – централизованную таблицу маршрутизации, где хранится отображение диапазонов или конкретных значений ключа на идентификаторы шардов. Фактически, это внешняя **маппинг-таблица**: по ключу шардинга можно найти номер шарда. Маршрутизатор сначала делает запрос в каталог соответствий, получает ответ – например, что значение `X` находится на шарде №3 – и затем направляет основной запрос на нужный узел.
2. Geographical Sharding – частный случай directory-based или range-подхода, когда ключом шардинга служит регион/локация данных. Например, глобальный сервис может разделить пользователей по географическому признаку: европейские пользователи на одном сервере (европейский датацентр), американские – на другом, азиатские – на третьем и т.д. 

### Consistent hashing (консистентное хеширование)

Идея - все участники образуют «хеш-кольцо».

- Хэшируем не `(key % N)`
- А ищем ближайший узел по часовой стрелке в кольце.

При добавлении шарда **-  п**ерекладываются **только часть ключей**, находящихся между соседями на кольце.

При удалении шарда **- п**ерекладываются ключи только его сектора.

### Cross-shard joins (джойны между шардами)

Самая большая проблема шардинга.

Джойны внутри шарда быстрые. Но join между шардами — это дорого.

### Стратегии выполнения cross-shard join’ов

1. Scatter-gather — запрос отправляется на все шарды → каждый возвращает частичные результаты → координатор делает join.

**Минусы:**

- Очень дорого.
- Легко DDOS-нуть весь кластер.
- Нельзя делать в real-time больших операций.
1. Execute on primary shard (routing by shard key)

![image.png](image%20621.png)

1. Denormalization (часто лучший вариант) - В NoSQL часто **просто хранят данные полностью или частично дублируя** → чтобы вообще не делать cross-shard joins.
2. Использование Materialized Views

MongoDB Atlas, Cassandra, DynamoDB Streams → позволяют заранее считать нужные данные внутри шарда.

1. Application-level join - перенос логики джоинов на уровень приложения

### Prons of sharding

- **Горизонтальное масштабирование нагрузки.** Это главное достоинство: шардирование позволяет распределить как объем хранимых данных, так и нагрузку чтения/записи между несколькими узлами. Вместо упорания в лимиты одного сервера (CPU, RAM, диск, сеть) мы можем добавлять новые машины в кластер, тем самым повышая общую производительность. В идеале, при правильном ключе, 2 сервера обрабатывают в ~2 раза больше транзакций, 10 серверов – в ~10 раз и т.д.
- **Масштабирование объема данных.** Даже если нагрузки нет, но данных очень много, шардирование позволяет **выйти за ограничения одного инстанса**. Например, если база достигла нескольких терабайт и уже не помещается на одном быстром диске или бэкап/восстановление занимает неприемлемо много времени, то разбивка на шарды решает эту проблему. Вместо одной таблицы на 2 млрд строк мы можем иметь 10 таблиц по 200 млн на разных узлах. Это уменьшает объем индексов на каждой ноде и может улучшать время ответа (поскольку индекс-деревья меньше, кэшируется больше эффективно и т.п.).
- **Локализация данных для пользователей.** Географическое шардирование, как упоминалось, позволяет хранить данные ближе к их потребителям. Это снижает latency запросов для распределенных по миру приложений. Пользователи из Европы общаются с европейским датацентром, из Азии – с азиатским, и все довольны быстротой отклика.
- **Комбинация с репликацией для высокой доступности.** Шардирование не исключает репликацию – наоборот, обычно **каждый шард реплицируется** на один или более стендбай-узлов для отказоустойчивости. Таким образом, при выходе из строя целого сервера теряется только один фрагмент данных, и даже он может быть поднят за счет реплики. Сама идея шардинга – убрать единую точку отказа, связанную с одной большой базой: если такой сервер упадет, вся система простаивает; а при шардах риск распределяется. (Конечно, появляется новая точка отказа – маршрутизатор или координатор, но и он может быть резервирован.)

### Cons of sharding

- **Сложность архитектуры и приложения.** Внедрение шардинга резко повышает сложность системы. Нужно управлять множеством баз вместо одной. Для разработчиков и DevOps это усложнение: больше компонентов – больше потенциальных точек отказа, больше места для ошибок конфигурации.
- **Ограничения на операции с данными.** Самая серьезная жертва – потеря некоторых возможностей привычной реляционной модели. Например, **глобальные уникальные ограничения** (UNIQUE, FOREIGN KEY на весь набор данных) становится либо невозможным, либо крайне неэффективным. Нельзя простым способом гарантировать уникальность значения на всех шардах. **Транзакции** – как уже обсуждалось, ACID-гарантии в пределах одного шарда есть, но *распределенные транзакции* тяжелы (2PC) или не используются, поэтому приходится проектировать систему так, чтобы минимизировать сценарии, требующие изменений на нескольких шардах сразу. **JOIN-ы** между данными разных шардов – тоже под вопросом: либо медленно через приложение, либо нужны механизмы вроде того же Citus, который способен часть джойнов выполнять распределенно.
- **Проблемы с агрегатами и аналитикой.** Любой запрос, охватывающий более одного шарда (например, “выбрать 100 последних событий из всей системы” или “подсчитать общее количество пользователей”) требует обращения ко всем шардам и слияния результатов. Это называется запросы “scatter-gather” (разбросать и собрать). Они могут сильно нагружать систему, особенно если делаются часто.
- **Распределение нагрузки может быть неравномерным.** Неправильный выбор shard key грозит феноменом **hot shard** – когда один узел оказывается узким местом, а остальные простаивают. Например, если шардировать интернет-магазин по `region`, то может выясниться, что 50% трафика – из одного региона, и один шард перегружен, а остальные на четверть мощности. Переразбиение данных (решардирование) – сложная процедура: надо либо переводить часть данных на другие узлы вручную, либо останавливать систему и перестраивать ключ. Поэтому на этапе дизайна крайне важно спрогнозировать объемы и паттерны доступа. Но в реальности нагрузки меняются, данные “перетекают”, потому полностью избежать hot-spot’ов сложно.